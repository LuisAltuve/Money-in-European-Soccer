{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaa0e657",
   "metadata": {},
   "source": [
    "# Final Project: Money in European Soccer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdd9810",
   "metadata": {},
   "source": [
    "<img src='assets/euro.jpg'>\n",
    "\n",
    "&emsp; As long as there is money to be made from professional sports, money will play a role in an individual's or team's success. In soccer, club owners will look to invest in a variety of ways in hopes to improve the team, leading to higher quality play and an increasing fanbase as a result. These investments can be used for improved training facilities, improved medical facilities, stadium renovations, hiring better coaches, hiring data analysts and scientists, etc... All these improvements across the club can have big impacts on a team's success but each game is still won and lost on the field and having the right players to make the most these advantages is critical. \n",
    "\n",
    "&emsp; For this reason, the biggest investments from owners often come in the form of an increased transfer budget so the team can pay a transfer fee to acquire a desirable player currently under contract with another team. Through this process of buying and selling players in the 'transfer market', the best players are brought in for large fees to the best clubs year after year. In this project, I aim to examine how the money spent on transfers affects the final league position of a team in their domestic league."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d4a03",
   "metadata": {},
   "source": [
    "## Part 1: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955d26af",
   "metadata": {},
   "source": [
    "&emsp; My data collection method of choice was to scrape the data from the popular soccer website <a href='Transfermarkt.us'>Transfermarkt.us</a>. This website holds all kinds of information involving various soccer players, clubs, and leagues with a focus on transfer news and records so I knew this website would have all the information I needed.\n",
    "<br><br>\n",
    "\n",
    "<img src='assets/transfermarkt.jpg'>\n",
    "\n",
    "<br>\n",
    "&emsp; From the <a href='www.transfermarkt.us/wettbewerbe/europa'>European leagues and cups</a> page I was able to create a dictionary of the top 15 most valuable European leagues with the links to their own pages on the site and I was off to the races."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b72f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for Scraping.\n",
    "\n",
    "# Libraries for obtaining and examining html.\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# Library for saving scraped data.\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717338d9",
   "metadata": {},
   "source": [
    "### Scraping Challenges\n",
    "\n",
    "#### Headers and Delay\n",
    "&emsp; Unfortunately, 'Transfermarkt.us' was a little particular about how it received a request so I had to disguise my requests with the 'headers' parameter and add a delay using python's 'time' library in between requests to seem like a normal web surfer. \n",
    "\n",
    "#### Top Ten Teams\n",
    "&emsp; I elected to scrape the results of the top ten teams (Based on current players' market value) of each league instead of all the teams in the league because promotion and relegation from and to the lower leagues meant that some of the other teams in the league would be more likely to be in and out of the 'First Tier'.\n",
    "\n",
    "#### Starting Rows\n",
    "&emsp; In my initial scraping attempts (Note that the pickle file is 'v3'), I thought that every page carrying the equivalent information of each team would be similarily structured. It turned out that most teams' transfer activity was tracked from the following season (24/25) but teams that already had a deal to sign a player for the 25/26 season started then. This lead to mismatched season and transfer information so I had to be more specific about where to start scraping from.\n",
    "\n",
    "<img src='assets/transfers.jpg'>\n",
    "\n",
    "After this mistake I made sure to confirm I was starting from the correct row for the 'transfer balance' data collection.\n",
    "\n",
    "#### League Position Information\n",
    "&emsp; With the transfer information secured, I also needed each team's final league position for each season. Fortunately there was a page on the site that held that information as well a bit of extra information about each season. The downside to using a transfers focused website to collect all my data was that information not pertaining to transfers was not as carefully archived.\n",
    "\n",
    "<img src='assets/league.jpg'>\n",
    "\n",
    "&emsp; For this team there is no record of the seasons between 95/96 and 00/01. Had I not noticed this problem, the data for the 95/96 season would've erroneously been assigned to the 99/00 season, 94/95 to 98/99, and so on. To solve this problem I implemented a way to only assign the info from the 'League rankings' page row if the season matched the season of the transfer information. This meant that some rows would have missing information about the league that season but would still have the correct transfer information and league position (if rows were missing it was because the team was in a lower league at the time and therefore their position could be marked as '≤10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f75333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request European leagues page.\n",
    "\n",
    "# Headers for disguising server request as a human.\n",
    "headers = {'User-Agent': \n",
    "           'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
    "\n",
    "url_prefix = \"https://www.transfermarkt.us\"\n",
    "\n",
    "# Requesting page from Transfermarkt containing links to European leagues.\n",
    "r_europe = requests.get(url_prefix + '/wettbewerbe/europa', headers=headers)\n",
    "\n",
    "if (r_europe.status_code == 200):\n",
    "    europe = bs(r_europe.content)\n",
    "else:\n",
    "    print('Error scraping Europe.')\n",
    "    \n",
    "# Extract links for examined leagues.\n",
    "europe = bs(r_europe.content)\n",
    "\n",
    "table = europe.find('tbody')\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "league_links = {}\n",
    "\n",
    "# Every row in the table had an extra row so the top 30 rows gave us the top 15 leagues.\n",
    "for row in rows[1:30]:\n",
    "    flag = row.find(class_=\"flaggenrahmen\")\n",
    "    if flag:\n",
    "        league_links[flag['alt']] = url_prefix + row.find('a')['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9541d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract transfer data for top ten teams in examined leagues and store them in one big dictionary.\n",
    "all_data = {}\n",
    "for league in league_links:\n",
    "    \n",
    "    # League page request.\n",
    "    time.sleep(2)\n",
    "    r_league = requests.get(league_links[league], headers=headers)\n",
    "    if (r_league.status_code == 200):\n",
    "        c_league = bs(r_league.content)\n",
    "    else:\n",
    "        print(f'Error scraping {league}.')\n",
    "        continue\n",
    "    \n",
    "    # League transfers page request.\n",
    "    time.sleep(2)\n",
    "    r_balance = requests.get(league_links[league].replace('startseite', 'transferbilanz'), headers=headers)\n",
    "    if (r_balance.status_code == 200):\n",
    "        c_balance = bs(r_balance.content)\n",
    "    else:\n",
    "        print(f'Error scraping {league} balance.')\n",
    "        continue\n",
    "              \n",
    "    # Finding league transfer balance table and the correct row to start from.\n",
    "    all_rows = c_balance.find(class_=\"items\").find(\"tbody\").find_all(\"tr\")\n",
    "    for i, row in enumerate(all_rows):\n",
    "        head = row.find(\"a\")\n",
    "        if head.text[-2:] == '23':\n",
    "            start_row = i\n",
    "            break\n",
    "    balance_rows = c_balance.find(class_=\"items\").find(\"tbody\").find_all(\"tr\")[start_row:]\n",
    "    \n",
    "    # Track progress while scraping by seeing which league is being scraped. \n",
    "    print(league)\n",
    "    \n",
    "    # Create a dictionary for each league in all data. \n",
    "    all_data[league] = {}\n",
    "    \n",
    "    # Finding team information from league table.\n",
    "    table = c_league.find(class_='responsive-table')\n",
    "    tbody = table.find('tbody')\n",
    "    rows = tbody.find_all(\"tr\")\n",
    "    \n",
    "    for i, row in enumerate(rows[:10]):\n",
    "        link = row.find('a')\n",
    "        team = link['title']\n",
    "        all_data[league][team] = {}\n",
    "               \n",
    "        # Transfer and league rankings page's urls are similar to team overview page url.\n",
    "        time.sleep(2)\n",
    "        r_team = requests.get(url_prefix + link['href'][:-14].replace('startseite', 'alletransfers'), headers=headers)\n",
    "        \n",
    "        time.sleep(2)\n",
    "        r_position = requests.get(url_prefix + link['href'][:-14].replace('startseite', 'platzierungen'), headers=headers)\n",
    "\n",
    "        if (r_team.status_code == 200) & (r_position.status_code == 200):\n",
    "            c_team = bs(r_team.content)\n",
    "            c_position = bs(r_position.content)\n",
    "        else:\n",
    "            print(f'Error scraping {team}.')\n",
    "            continue\n",
    "        \n",
    "        # Isolate useful information from webpages.\n",
    "        season_rows = c_position.select(\"tbody tr\")[2:]\n",
    "        \n",
    "        # Find the correct row to start from.\n",
    "        all_rows = c_team.find_all(class_='row')\n",
    "        for i, row in enumerate(all_rows):\n",
    "            head = row.find(\"h2\")\n",
    "            if head:\n",
    "                if head.text.strip()[-2:] == '23':\n",
    "                    start_row = i\n",
    "                    break\n",
    "        transfer_rows = c_team.find_all(class_='row')[start_row:]\n",
    "                \n",
    "        # Season row information to handle missing 'League rankings' rows.\n",
    "        season_counter = 0\n",
    "        season_max = len(season_rows)\n",
    "        \n",
    "        # League and transfer data going back 30 seasons.\n",
    "        for i, season in enumerate(transfer_rows[:30]):\n",
    "            \n",
    "            # Year determined by the last two chars (E.g. '22/23')\n",
    "            year = season.find(\"h2\").text.strip()[-2:]\n",
    "            transfer_tables = season.find_all(class_='box')\n",
    "            \n",
    "            # If there was no data for the revenue or spend of a season, that value will be set to 0. \n",
    "            transfer_revenue = transfer_tables[1].select('tfoot td')\n",
    "            revenue = '0' \n",
    "            if len(transfer_revenue) > 0:\n",
    "                revenue = transfer_revenue[0].text\n",
    "                \n",
    "            transfer_spend = transfer_tables[0].select('tfoot td')\n",
    "            spend = '0' \n",
    "            if len(transfer_spend) > 0:\n",
    "                spend = transfer_spend[0].text\n",
    "            \n",
    "            # Changing season counter back to '0' means the years will never match again.\n",
    "            if season_counter == season_max:\n",
    "                season_counter = 0\n",
    "            \n",
    "            # Missing 'League ranking' rows handled here.\n",
    "            season_info = season_rows[season_counter].find_all('td')\n",
    "            \n",
    "            # If the years match, set the variables to the information found in the row.\n",
    "            if year == season_info[0].text[-2:]:\n",
    "                season_counter += 1\n",
    "                goals = season_info[7].text\n",
    "                competition = season_info[3].text\n",
    "                position = season_info[10].text\n",
    "                wins = season_info[4].text\n",
    "                ties = season_info[5].text\n",
    "                losses = season_info[6].text\n",
    "            \n",
    "            # If the years don't match, set all variables except 'competition' and 'position' to NaN.\n",
    "            else:\n",
    "                goals = np.nan\n",
    "                competition = 'Not First'\n",
    "                position = '≤10'\n",
    "                wins = np.nan\n",
    "                ties = np.nan\n",
    "                losses = np.nan\n",
    "                \n",
    "            # Create dictionary for each season that holds all data for that season.\n",
    "            all_data[league][team][year] = {}\n",
    "            \n",
    "            # Store variables in dict.\n",
    "            all_data[league][team][year]['revenue'] = revenue\n",
    "            all_data[league][team][year]['spent'] = spend\n",
    "            \n",
    "            all_data[league][team][year]['goals'] = goals\n",
    "            all_data[league][team][year]['competition'] = competition \n",
    "            all_data[league][team][year]['position'] = position\n",
    "            all_data[league][team][year]['wins'] = wins\n",
    "            all_data[league][team][year]['ties'] = ties\n",
    "            all_data[league][team][year]['losses'] = losses\n",
    "            \n",
    "            # Total league transfer spend this season.\n",
    "            all_data[league][team][year]['league_spent'] = balance_rows[i].find_all(\"td\")[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f733fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scraped data dictionary as pickle file.\n",
    "with open('all_datav3.pickle', 'wb') as handle:\n",
    "    pickle.dump(all_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78fd44f",
   "metadata": {},
   "source": [
    "### Final Thoughts \n",
    "&emsp; A data analysis is only as good as its data. If I were to do it over, I would source more information about each season from other websites in addition to the transfer information from 'Transfermarkt.us'. Including more leagues, teams, and seasons from each country, instead of just the top 10, would've been challenging but might've provided a more complete picture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
